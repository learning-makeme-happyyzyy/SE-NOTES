#+TITLE: The Rise of AI Teammates in Software Engineering (SE) 3.0: How Autonomous Coding Agents Are Reshaping Software Engineering

* 研究背景与动机

** SE 3.0时代的到来
- SE 1.0：传统手工编码时代，无AI辅助
- SE 1.5：预测性编码（代码补全、智能提示）
- SE 2.0：AI辅助软件工程（LLM生成代码）
- SE 3.0：自主AI队友时代，能独立完成端到端开发任务

** 研究动机
- 现有研究多为理论推测，缺乏真实世界中AI队友行为的实证数据
- 需要回答关键问题：
  - 自主编码代理如何影响开发者生产力？
  - AI生成的代码是否可被合并？
  - 人类团队如何适应自主编码代理的存在？
  - 当前基准测试是否能反映AI在真实环境中的表现？

* 主要贡献

** AIDev数据集
- 规模：456,535个由自主编码代理创建的PR
- 覆盖范围：61,453个GitHub仓库，47,303名开发者
- 包含的AI代理：OpenAI Codex、Devin、GitHub Copilot、Cursor、Claude Code
- 数据结构：丰富的元数据，包括PR时间线、代码审查、提交详情、关联问题等

** 实证研究发现
- 系统分析了AI代理在真实项目中的表现、审查动态和代码质量
- 揭示了基准测试表现与现实效果之间的显著差距

** 未来研究方向
- 提出了9个具体的研究方向，指导SE与AI交叉领域的未来探索

* 关键研究发现

** 生产力与接受率
- AI代理已成为不可或缺的协作者，55%+的PR涉及功能开发或修复
- AI-PR接受率显著低于人类（如Codex 64% vs 人类76.8%）
- 文档任务是AI的强项，接受率超过人类基准

** 效率与审查动态
- GitHub Copilot完成速度极快（50%的PR在13分钟内完成）
- OpenAI Codex PR审查速度快10倍（0.3小时 vs 人类3.9小时）
- 人类仍是主要审查者，但机器人审查比例在AI-PR中显著上升

** 代码质量与归属
- AI更倾向于简单、模板化代码，较少引入复杂度变化
- 作者归属不明确，部分AI代理不标注贡献者，影响可追溯性
- AI代理表现出语言偏好（如Codex偏向Python，Copilot偏向C#）

* 未来研究方向

** 基准测试与评估
- 开发基于真实工作流的集成导向基准测试
- 分析被拒PR以识别AI失败模式

** 系统优化
- 延迟感知的AI代理编排
- 设计智能PR分流系统管理审查工作量

** 质量与协作
- 评估AI代码的长期质量
- 研究人-AI协作中的任务规划阶段
- 研究编程语言特性对AI效果的影响

** 审查流程
- 理解并降低审查AI代码的人力成本
- 改进AI代码审查的质量与流程

* 未来展望：SE 3.0方法论

** 软件仓库作为AI训练环境
- 将GitHub仓库视为强化学习环境
- 使用真实信号（PR合并、测试通过）作为奖励

** 动态基准测试
- 取代静态基准测试（如SWE-bench）
- 建立实时排行榜，反映真实项目集成效果

** 新工程方法论
- 需要新的协作框架、审查机制和治理模型
- 重新定义敏捷、DevOps等实践以适应人-AI混合团队

* 结论

- AIDev提供了首个大规模实证基础，证明自主编码代理时代已经到来
- AI代理能显著加速代码贡献，但在接受率和代码复杂性方面与人类存在差距
- 数据集将支持下一代软件工程研究，推动SE 3.0从理论走向实践

* 可以提出的问题：
1. 混淆变量： 研究发现AI-PR的接受率更低。我们如何确定这是因为代码质量差，而不是因为人类审查员对AI生成的代码抱有固有的不信任或更高的审查标准？这一点可能会影响因果推断
Regarding potential confounding variables: The empirical findings indicate a systematically lower acceptance rate for Agentic-PRs. 
How can we ascertain that this is primarily due to inferior code quality, as opposed to inherent distrust or a heightened scrutiny standard applied by human reviewers to AI-generated code? 
This ambiguity presents a challenge to establishing a clear causal interpretation of the results.

#+TITLE: Is Refactoring Always a Good Egg? Exploring the Interconnection Between Bugs and Refactorings
本文通过实证研究深入探讨了代码重构与软件缺陷之间的复杂关系。

* 研究背景与动机

** 传统认知
- Bug修复：纠正性修改，旨在消除程序缺陷
- 代码重构：行为保持的代码改进，旨在提升内部质量而不改变功能
- 传统观点认为这两种活动应该独立进行，且重构不应引入缺陷

** 研究动机
- 实际开发中，开发者对重构存在顾虑，担心引入缺陷
- 现有研究对重构与缺陷关系的结论不一致
- 需要基于大规模实证数据探究重构与缺陷的真实关系

* 研究问题

** RQ1：Bug修复提交中是否常见混杂重构更改？
- 探究开发者在修复缺陷时是否同时进行重构

** RQ2：重构操作是否出现在引入缺陷的代码修改中？
- 分析重构是否真的可能引入新的缺陷

** RQ3：在引入缺陷的提交中，哪些重构类型最为常见？
- 识别高风险的重构类型，为开发者提供预警

* 研究方法

** 数据来源
- SmartSHARK 2.2 数据集
- 涵盖96个Java项目
- 包含提交标签、重构操作、缺陷引入信息

** 分析工具
- 重构检测：RMiner工具（精度98%，召回率87%）
- 缺陷引入识别：基于SZZ算法
- 提交级别关联分析

** 分析方法
- 通过提交ID关联重构与缺陷记录
- 统计共现频率
- 识别高风险重构类型

* 主要研究发现

** RQ1：Bug修复提交中的重构混杂情况
- 41/96个项目存在重构与bug修复混杂的提交
- 平均21%的bug修复提交包含重构操作
- 最高比例：Calcite项目（41%）
- 但仅有10%的重构操作与bug修复混杂

** RQ2：重构在缺陷引入提交中的出现情况
- 平均54%的缺陷引入提交包含重构操作
- 比例范围：20%（commons-validator）到71%（Calcite）
- 重构与缺陷引入存在显著共现关系

** RQ3：高风险重构类型识别

*** 高频出现类型（按次数）
- Change Variable Type（652次）
- Extract Method（454次）
- Change Return Type（338次）

*** 高风险类型（按比例R1%）
- Extract Subclass（33%出现在缺陷引入提交中）
- Replace Attribute（29%）
- Move and Rename Attribute（28%）

* 结论与启示

** 主要结论
- 重构与缺陷活动在实践中并非独立
- 重构并非总是"安全"的行为保持操作
- 特定重构类型具有较高风险

** 实践启示
- 重构时应加强验证与测试，特别是高风险类型
- 避免在修复缺陷时混杂不相关的重构
- 提高对重构潜在风险的认识

** 研究局限性
- 基于提交级别的共现分析，未建立因果关系
- 依赖检测工具的准确性（RMiner、SZZ）
- 数据集局限于Java项目

* 未来工作

** 定性研究
- 深入分析重构与缺陷之间的因果关系
- 探究重构引入缺陷的根本机制

** 扩展研究
- 扩展到更多编程语言和项目类型
- 开发重构风险评估工具
- 建立重构最佳实践指南

* 总结

本研究通过实证分析挑战了"重构总是安全"的传统观念，揭示了重构与缺陷之间的复杂关联，为开发者理解和管理重构风险提供了重要依据。

* 可以提出的问题：
“最安全”的重构是什么？ 论文重点指出了高风险重构，但从表3看，像“移动类”这样的重构在缺陷引入提交中出现的比例较低（5%）。这是否意味着某些重构实际上是相对安全的？
The paper prominently highlights high-risk refactoring types. 
However, as seen in Table 3, refactorings like Move Class have a relatively low presence in bug-inducing commits (R1% of only 5%). 
Does this imply that certain refactoring operations are, in fact, relatively safe? 
What patterns or characteristics might these lower-risk refactorings share, and could this inform the development of safer refactoring practices?

#+TITLE: 论文笔记：An Empirical Study of End-user Programmers in the Computer Music Community
#+AUTHOR: Gregory Burlet, Abram Hindle
#+DATE: 2015

* 摘要
计算机音乐家是使用可视化编程语言（如 Max/MSP, Pure Data）的**终端用户程序员**。
想象一下，传统的音乐家弹的是钢琴、吉他，而计算机音乐家“弹”的是电脑。他们不只是用现成的音乐软件（比如GarageBand）来拖拽音效，而是像一个程序员一样，通过“连接积木”的方式来从零开始创造声音和音乐。
-“积木”：就是软件里的各种小模块，有的负责发出声音（比如一个模拟正弦波的振荡器），有的负责控制音量，有的负责接收键盘或MIDI控制器的信号。
-“连接”：就是用虚拟的线把这些模块连起来，让数据（比如音频信号、控制信号）在它们之间流动。
他们使用的“画布”就是像 Max/MSP 或 Pure Data 这样的可视化编程语言。所以，他们既是音乐家，也是程序员，我们称他们为 “用电脑作曲的程序员”。


本研究通过多层面分析揭示了他们的开发实践：
1.  与普通开发者相比，他们的代码库有：**更少提交**、**提交频率更低**、**更多周末提交**，但**问题报告数和贡献者数相似**。（“他们更随性，更像是周末艺术家”）
2.  源代码分析发现，绝大部分代码可由重复的代码片段重建。（“他们的代码‘复制粘贴’率极高”）
3.  调查和访谈结果佐证了上述发现。（“他们不太爱用‘高级工具’，但协作精神不差”）
结论：软件工程有许多途径可以帮助这个终端用户程序员社区。

这篇论文就像一个“针对‘用电脑作曲的程序员’这个特殊群体的社会调查报告”。它想搞清楚一个问题：这群既是艺术家又是程序员的人，他们的工作方式和普通的软件工程师有什么不同？

* 1. 引言
** 背景
- 计算机音乐家是**终端用户程序员**，使用可视化编程语言（如 Max/MSP, Pure Data）创作音乐。
- 他们面临与专业程序员相似的软件工程挑战（如数据流、调试、测试、API调用），但技术能力参差不齐。
- 可视化音乐编程语言通过排列**对象**（Objects）并用**连线**（Patchcords）连接它们来工作。一个文件称为一个**程序**（Patch）。

** 研究问题
- 我们不清楚计算机音乐家如何编程、分享和协作。
- 他们是否使用软件工程工具（如版本控制、问题追踪器）？
- 他们在开发过程中遇到什么问题？现有的软件工程实践和工具能否帮助他们？

* 2. 相关工作
** 2.1 终端用户可视化编程
- 终端用户程序员数量庞大，其目标是实现个人领域内的目标，而非受雇开发软件。
- 计算机音乐家属于终端用户，他们使用专门的、音乐导向的编程语言进行个人创作。
- 可视化编程语言通过图形化隐喻降低学习曲线，并提供实时反馈。
- 已有对其他终端用户社区（如Yahoo! Pipes, CoScripter, 电子表格用户）的研究，但对计算机音乐社区尚无实证研究。

** 2.2 软件仓库挖掘
- GitHub 是流行的公开软件项目集合。
- GHTorrent 项目提供了从 GitHub 提取的数据集，用于研究。

** 2.3 克隆检测
- 克隆检测可用于促进代码重用、为新手提供参考、定位需要重构的代码。
- 已有研究对类似的可视化编程语言（如Matlab Simulink）进行克隆检测。
- Gold et al. (2011) 对 Max/MSP 教程程序进行了克隆检测，发现86%的连接对象是最低粒度的克隆。但未研究社区开发的程序。

* 3. 挖掘 Git 仓库
** 目标：分析计算机音乐家与普通开发者在开发实践上的差异。

** 3.1 数据集
- **计算机音乐数据集**：从GHTorrent中查询得到819个主要包含Max/MSP或Pure Data文件的仓库。
  | 语言    | 仓库数 | 程序数  | 对象数   |
  |---------|--------|---------|----------|
  | Max/MSP | 168    | 15,016  | 565,705  |
  | Pure Data | 651  | 103,465 | 2,521,573 |
  | 总计    | 819    | 118,481 | 3,087,278 |
- **随机样本数据集**：从GHTorrent中随机抽取819个仓库作为对照。

** 3.2 假设与访谈反馈
针对以下5个假设，访谈了15位计算机音乐家：
1.  H1: 提交次数更少 :: 7同意，1反对，7不确定
2.  H2: 周末提交更多 :: 4同意，2反对，9不确定
3.  H3: 提交频率更低 :: 5同意，1反对，9不确定
4.  H4: 问题报告更少 :: 7同意，3反对，5不确定
5.  H5: 贡献者更少   :: 8同意，1反对，6不确定

** 3.4 结果
使用Wilcoxon秩和检验与Cliff's Delta效应量进行验证 (α=0.01)：
| 假设 | 结果 | p值 | 效应量 | 结论 |
|------|------|-----|--------|------|
| H1: 提交次数更少 | ✅ 支持 | 1.133e-13 | 小 | 计算机音乐家提交更少 |
| H2: 周末提交更多 | ✅ 支持 | 7.091e-5 | 小 | 计算机音乐家周末提交更多 |
| H3: 提交频率更低 | ✅ 支持 | ≈0 | 中等 | 计算机音乐家提交频率更低 |
| H4: 问题报告更少 | ❌ 拒绝 | 0.214 | 可忽略 | 两者问题报告数无差异 |
| H5: 贡献者更少 | ❌ 拒绝 | 0.4673 | 可忽略 | 两者贡献者数无差异 |

** 总结：计算机音乐家提交更少、更不频繁、更多在周末，但问题报告和协作程度与普通开发者无显著差异。

* 4. 克隆检测
** 目标：分析计算机音乐家代码中的重复结构。

** 4.1 克隆检测算法
- 定义了两种克隆粒度：
  - **DF1克隆**：子图对象类型、字面值参数、连接方式完全相同。
  - **DF2克隆**：子图对象类型、连接方式相同，参数可不同。
- 算法：将程序解析为图，深度优先遍历（深度≤8），提取路径属性，生成JSON文本并哈希，通过哈希比较检测克隆。
- 验证：在Gold et al.使用的教程程序数据集上验证，结果相似。

** 4.3 结果
在118,481个程序中发现：
| 克隆类型 | 克隆数量 | 路径总数 | 克隆比例 |
|----------|----------|----------|----------|
| DF1      | 9,798,031 | 10,985,064 | 89.2% |
| DF2      | 10,462,725 | 10,985,064 | 95.2% |
- 结论：计算机音乐家的代码重复率极高。

** 4.4 值得注意的克隆示例
- (a) 包络跟随器去归一化
- (b) 触发多个bang的loadbang
- (c) Pure Data中模拟loadmess
- (d) 恒等函数（可能为预留功能）
- (e) 过于简单的函数（如衰减）
- (f) 无操作参数（如乘1）或静音参数
- (g) 单行表达式 vs 链式数学运算
- (h) 高通+低通滤波器（可用现成的带通滤波器替代）
- (i) 使用外部对象库简化常见功能
- (j) 使用魔数（如MIDI最大值127）和数学常数（如π）

** 工具启示：可开发代码补全、代码审查、代码搜索等工具来帮助计算机音乐家。

* 5. 计算机音乐家调查与访谈
调查了175位计算机音乐家，访谈了15位。

** 5.1 经验
- 经验年限分布广泛。
- 自评水平：15初学者，84中级，74高级。

** 5.2 动机
- 65% 作为业余爱好。
- 40% 为他人或公司开发。
- 访谈：音乐项目通常高度个人化，遵循单一音乐家的创作愿景。

** 5.3 编程方法与版本控制
- **常用语言**：Max/MSP 和 Pure Data 是最常用的前两名。
- **编写测试**：仅30%编写测试。可能因声音属性难以量化断言。
- **代码注释**：大多数会注释关键部分。数据集中有16.4%的对象是注释对象。
- **使用版本控制**：54%使用。前五名版本控制系统中有四个是Git。
- **不使用版本控制的原因**：
  - 还在学习如何使用
  - 觉得没必要（版本备份未造成大问题）
  - 代码不与他人协作，版本管理非优先事项

** 5.4 开发支持
- 仅26%使用Stack Overflow。
- 54%订阅邮件列表。
- 求助路径：先使用搜索引擎，再通过邮件列表求助社区。
- 启示：社区可能受益于一个专门的问答网站。

* 6. 有效性威胁
- **抽样偏差**：GitHub上的公共仓库可能不能代表所有计算机音乐家；随机样本也未过滤非软件项目。
- **克隆检测**：混合分析Max/MSP和Pure Data可能因同名对象功能不同导致克隆数被低估。
- **调查与访谈**：招募渠道可能偏向特定子社区。

* 7. 未来方向
- **开发专用工具**：
  - 代码补全
  - 代码审查
  - 代码搜索
  - 代码高亮与导航
  - 可视化代码聚类
  - 音乐软件测试框架
- **改进版本控制**：适配可视化、基于程序的源代码的差异比较与合并。
- **教育**：向计算机音乐家普及软件工程工具和方法论的价值与使用。

* 8. 结论
- **开发实践**：计算机音乐家提交更少、更不频繁、更多在周末，但问题报告和协作程度与普通开发者无差异。
- **代码重复**：代码重复率极高（DF1: 89.2%, DF2: 95.2%），许多克隆是现有功能的重复实现。
- **工具使用**：许多人不用版本控制，缺乏专用的支持网站，依赖邮件列表。
- **未来工作**：需要教育和开发工具来帮助这个终端用户社区及其他使用可视化编程语言的社区。

可以提出的问题：
1.What were the criteria for selecting the analyzed music software projects? 
Were they chosen based on popularity, functionality, or language similarity?

2. How was the “computer music repository” dataset specifically identified and filtered from GHTorrent? 
Were any manual validation steps taken to ensure repositories were indeed music-related and actively developed?
计算机音乐仓库的数据集是如何从 GHTorrent 中具体识别和筛选的？是否有人工验证步骤来确保这些仓库确实与音乐相关且处于活跃开发状态？

3. Why was the path depth limited to 8 in the clone detection algorithm?
Was this value empirically determined, and how might it affect the detection of larger clone structures?
为什么在克隆检测算法中将路径深度限制为 8？这个值是经验确定的吗？它如何影响对更大克隆结构的检测？

#+TITLE: Can LLMs Replace Manual Annotation of Software Engineering Artifacts?
#+AUTHOR: Toufique Ahmed, Premkumar Devanbu, Christoph Treude, Michael Prade
#+DATE: 2024

* 核心问题
** 能否用大语言模型来替代昂贵的人工标注，用于软件工程产物的评估？

* 摘要
- **背景**：软件工程研究中的人工实验成本高昂（例如，聘请专业开发者每小时60美元），且难以执行。
- **机遇**：大语言模型在多项任务上展现出接近人类的能力，且调用成本极低。
- **研究**：将6个先进的LLM应用于10个来自5个数据集的标注任务，比较LLM与人类标注员之间的一致性。
- **发现**：
  1. 在一些任务上，LLM与人类的一致性接近人类之间的一致性。
  2. **模型间一致性** 可以用来预测一个任务是否适合使用LLM。
  3. **模型置信度** 可以用来选择LLM能安全替代人类的特定样本。
- **结论**：LLM可以部分替代人工标注，形成一个**混合的人机评估**模式，显著降低成本。

* 1. 引言
** 研究动机
- 软件工程工具（如代码摘要、静态分析）的最终价值取决于人类的判断。
- 人工评估成本高、耗时长，且难以找到代表性样本（如专业开发者）。
- LLMs的出现提供了一个潜在的低成本替代方案。

** 核心研究问题
- When, and how, can human subject responses be safely replaced by LLMs, in a mixed human-LLM evaluation scenario?

* 2. 方法论
** 2.1 任务与数据集
选择了5个数据集，涵盖10个标注任务：
1. 自动代码摘要 :: 评估生成摘要的准确性、充分性、简洁性、相似性。
2. 名称-值一致性 :: 判断变量名与其值是否匹配。
3. 因果关系提取 :: 从需求文档中判断句子是否包含因果关系。
4. 语义相似性 :: 判断两个函数在目标、操作、效果上是否相似。
5. 静态分析警告 :: 判断代码变更是否真正修复了静态分析警告。

** 2.2 使用的模型
- **闭源模型**：GPT-4, Claude-3.5-Sonnet, Gemini-1.5-Pro, GPT-3.5
- **开源模型**：Llama3 (70B), Mixtral (8x22B)

** 2.3 研究问题
- RQ1 :: LLM与人类标注员的一致性水平如何？
- RQ2 :: 如何判断一个任务是否不适合使用LLM？
- RQ3 :: 如何判断LLM对单个样本的回答是否可靠？
- RQ4 :: 能节省多少人工标注成本？

** 2.4 评估方法
- 使用 **Krippendorff‘s α** 作为一致性度量指标。
- 比较三类一致性：
  - Human-Human (H-H)
  - Human-Model (H-M)
  - Model-Model (M-M)

* 3. 结果与发现
** RQ1: LLM与人类的一致性水平
| 任务类别             | H-H一致性 | H-M一致性 | 结论                                     |
|----------------------|-----------|-----------|------------------------------------------|
| 代码摘要（多数任务） | 中-高     | 与H-H相当 | LLM可作为有效替代                        |
| 名称-值一致性        | 高        | 与H-H相当 | LLM可作为有效替代                        |
| 语义相似性           | 高        | 高        | LLM可作为有效替代                        |
| 因果关系提取         | 中        | 低        | LLM表现不佳                             |
| 静态分析警告         | 高        | 低        | LLM表现不佳                             |

** RQ2: 如何判断任务是否适合LLM？
- **关键发现**：Model-Model一致性与Human-Model一致性**强正相关**。
- **实践指南**：可以先计算多个LLM在目标任务上的M-M一致性。
  - 如果 > 0.5 → 任务适合使用LLM。
  - 如果 ≤ 0.5 → 任务不适合或需谨慎使用。

** RQ3: 如何判断单个样本的LLM回答是否可靠？
- **关键发现**：LLM的**输出概率（置信度）** 是一个有效的指示器。
- **实践指南**：优先选择LLM输出置信度高的样本来替代人工标注，这比随机选择能更好地维持整体一致性。

** RQ4: 能节省多少人力？
- 通过有选择地用LLM替代**每个样本的一个**人类标注，可以在**7/10个任务**中保持原有的一致性水平。
- 总体标注工作量**最高可节省约33%**。
| 数据集/任务          | 可节省的总体工作量 |
|----------------------|--------------------|
| 代码摘要（多数任务） | 33%                |
| 代码摘要（相似性）   | 16.5%              |
| 名称-值一致性        | 9%                 |
| 因果关系             | 30%                |
| 语义相似性           | 33%                |
| 静态分析警告         | 25%                |

* 4. 讨论与建议
** 提出的决策流程
1. 用少量示例（3-4个）测试多个LLM，计算**模型间一致性**。
   - 若 > 0.5 → 可安全地用LLM替代**每个样本的一个**人类标注。
   - 若 ≤ 0.5 → 进入下一步。
2. 对于一致性低的任务，仅对LLM**输出置信度高**的样本使用其标注结果。

** 重要警示
- **不应完全替代所有人类**：用LLM完全取代所有人类标注员可能会人为地提高一致性，但无法反映真实的人类认知差异。
- **少量示例学习至关重要**：零样本学习效果不佳。

* 5. 局限性
- 只研究了10个任务和6个模型。
- 某些数据集的标注员数量少或数据分布不均。
- 存在LLM在训练中已见过部分测试数据的风险。
- 未研究自由形式的标注任务，也未探讨模型输出偏差或人口统计学问题。

* 6. 结论
- LLM可以部分替代人工标注，形成一个**混合的人机评估**模式。
- **模型间一致性**是判断任务适用性的有效预测指标。
- **模型置信度**是选择可靠样本的有效方法。
- 这项工作是迈向软件工程中混合人-LLM评估的第一步。

* 可以提出的问题：
1.In RQ3/RQ4, the conclusion that inter-rater agreement does not change "significantly" is crucial. 
What statistical test was used to determine significance, and what was the p-value threshold? This is not explicitly stated.
在RQ3/RQ4中，“评估者间一致性没有发生‘显著’变化”这一结论至关重要。是使用了哪种统计检验来确定显著性的，p值的阈值是多少？这一点没有明确说明。

2.The prompts for different tasks (especially complex ones like static analysis warnings) likely varied in structure and detail. 
How was the prompt engineering process conducted to ensure fairness and optimal performance across all tasks and models? 
Were prompts optimized for each model?
针对不同任务（特别是像静态分析警告这样的复杂任务）的提示词，其结构和细节很可能不同。
提示词工程的过程是如何进行的，以确保在所有任务和模型间的公平性和最佳性能？提示词是否为每个模型单独优化过？


#+TITLE: GreenHub Farmer: Real-world data for Android Energy Mining
#+AUTHOR: Hugo Matalonga, Bruno Cabral, Fernando Castor, Marco Couto, Rui Pereira, Simao Melo de Sousa, Joao Paulo Fernandes

* 研究背景与动机
** 问题现状
- 90%的用户存在低电量焦虑（low battery anxiety）
- 电池寿命是影响消费者满意度的最重要因素之一
- 应用能耗过高是导致差评的主要原因

** 研究挑战
*** 开发者面临的问题
- 缺乏系统化的能耗分析工具
- 测试场景复杂，需要多设备多环境
- Android生态系统的高度异构性

*** 用户面临的问题
- 缺乏技术背景和工具进行能耗分析
- 无法进行跨设备比较
- 难以制定有效的节能策略

** 环境意义
- 移动设备全球能耗已超过航空业
- 节能对可持续发展具有重要意义

* GreenHub平台架构

** BatteryHub（数据采集端）
*** 功能特性
- Android应用，通过Google Play Store分发
- 监听系统广播事件（电池状态变化等）
- 采集设备状态样本

*** 数据采集内容
- 传感器状态
- 内存使用情况
- 电池温度/电压
- 运行中的应用
- 网络状态
- 设备型号信息

*** 隐私保护
- 使用随机唯一标识符
- 不收集个人身份信息（电话号码、位置、IMEI等）

** Farmer（数据集与服务器）
*** 技术架构
- PHP + Laravel框架
- MariaDB关系型数据库
- 提供Web仪表盘界面

*** 数据集规模（截至2018年12月）
- 数据样本: 12.22 million+
- 唯一设备: 36,933台
- 品牌数量: 964个
- 设备型号: 5,665种
- Android版本: 36个
- 覆盖国家: 160个

*** 数据访问方式
- MariaDB数据库dump文件
- CSV格式数据集（5.2GB压缩包）
- 在线Web仪表盘

** API & Lumberjack（数据访问工具）
*** RESTful API
- 提供程序化数据访问接口
- 支持数据模型扩展

*** Lumberjack命令行工具
- 支持灵活查询和过滤
- 便于快速数据原型开发
- 示例查询：获取Google品牌设备列表

* 研究方向与应用场景

** 面向开发者的研究方向
1. 应用对电池消耗的影响分析
2. 不同版本间的能耗演变
3. 不同环境下的能耗变化（OS + 其他应用）
4. 与同类应用的能耗对比
5. 不同用户群体的能耗差异
6. 系统电池管理策略对应用运行时间的影响

** 面向用户的研究方向
7. 与相似使用习惯用户的能耗对比
8. 节能使用模式的识别与复制
9. 传感器使用对电池的影响
10. 应用组合运行的能耗优化
11. 高能耗应用的识别
12. 节能替代应用推荐
13. 基于使用习惯的电池寿命预测

** 已有研究案例
- 不同OS版本和品牌的充放电行为分析
- 手机游戏对电池消耗的影响量化（Fernando Castor团队）
- 用户感知与实际能耗的相关性研究（Ivan Machado团队）

* 数据价值与未来潜力

** 数据特色
- 真实世界使用数据
- 大规模、多样化样本
- 跨设备、跨国家覆盖
- 持续增长的数据集

** 技术潜力
- 适合机器学习与AI分析
- 支持能耗模式识别
- 异常检测与个性化推荐

** 社会价值
- 帮助开发者优化应用性能
- 指导用户改善使用习惯
- 促进移动设备节能环保

* 结论与贡献

** 主要成果
- 构建了大规模Android能耗数据集GreenHub Farmer
- 开发了完整的数据采集、存储、访问平台
- 数据公开可用，支持协作研究

** 开放邀请
- 邀请研究者和开发者使用数据集
- 鼓励对GreenHub项目的贡献和协作
- 持续完善数据分析和应用场景

* 参考文献
- 共30篇参考文献，涵盖能耗分析、移动开发、用户行为等领域
- 包括学术论文、技术报告和行业分析

* 可以提出的问题
1. "tracks system event broadcasts such as changes to the battery's state, and when such an event occurs, it obtains a sample"----原文
"电池状态变化"的具体定义：是指每1%的电量变化，还是包括充电状态、温度等任何变化？
Could you precisely define what constitutes a change in the battery's state? Does it refer specifically to a change in the battery level (e.g., a 1% change), 
or does it encompass any change in state, such as charging status, temperature, voltage, or other properties?

2. 非事件触发的采样：是否在无状态变化时也有定期采样，以确保数据连续性？
Is there also a mechanism for non-event-triggered sampling or periodic sampling when the battery's state is stable to ensure a continuous time series of data 
and to capture the device's state even in the absence of state changes?


#+TITLE: Language Models in Software Development Tasks: An Experimental Analysis of Energy and Accuracy
#+AUTHOR: Negar Alizadeh, Boris Belchev, Nishant Saurabh, Patricia Kelbert, Fernando Castor

* 研究背景与动机
** 现状与挑战
- 生成式AI编码助手（如ChatGPT、GitHub Copilot）在软件开发中广泛使用
- 第三方API存在数据隐私、安全和订阅成本问题
- 本地部署LLM成为趋势，但面临高能耗和硬件要求挑战

** 研究空白
- 现有研究主要关注模型训练阶段的能耗，忽略推理阶段的长期影响
- 代码LLM评估多关注准确性，缺乏对能耗效率的综合分析
- 缺乏针对软件开发任务的系统性能耗评估

* 研究问题
** RQ1: 任务类型对能耗的影响
- 不同模型在不同软件开发任务中的能耗差异如何？

** RQ2: 能耗与准确性的权衡关系
- 模型在执行同一任务时，能耗效率与准确性是否存在权衡？

** RQ3: 架构特征与能耗的关联
- 模型架构特征是否与能耗有系统关联？

** RQ4: 专用模型与通用模型的对比
- 通用模型与代码专用模型在能耗与准确性上有何差异？

* 实验设计
** 模型选择
*** 选择标准
- 流行度（HuggingFace下载量）
- 开发者信誉
- 公开可用性
- GGUF格式兼容性

*** 模型范围
- 18个模型家族
- 3种精度格式：FP16、Q8、Q4
- 参数规模：不超过200亿参数
- 包含通用模型和代码专用模型

** 硬件配置
*** 笔记本环境（RTX 3070）
- GPU: NVIDIA GeForce RTX3070 (8GB)
- CPU: Intel Core i7-1158OH
- 内存: 31GB
- 用途: 小模型量化实验

*** 服务器环境（A100）
- GPU: NVIDIA A100 PCIe (80GB)
- CPU: 2×AMD 7313
- 内存: 1TB
- 用途: 全精度与大模型实验

** 任务与数据集
*** 任务类型
1. 代码生成（Code Generation）
2. 缺陷修复（Bug Fixing）
3. 文档字符串生成（Docstring Generation）
4. 测试用例生成（Test Generation）

*** 数据集
- HumanEvalPack（Python子集）
- 包含164个编程问题
- 扩展版本，支持多任务评估

** 测量方法
*** 能耗测量工具
- GPU能耗: pyNVML（10Hz采样）
- CPU能耗: pyRAPL（1Hz采样）

*** 评估指标
- 准确性: pass@1
- 能耗: 瓦时（Wh）
- 效率: tokens/J
- 测试覆盖率: 语句和分支覆盖率

* 主要发现
** RQ1: 任务类型对能耗的影响
*** 能耗排序（从高到低）
1. 测试生成（37.94Wh均值）
2. 缺陷修复（29.69Wh均值）
3. 文档生成（19.12Wh均值）
4. 代码生成（13.46Wh均值）

*** 关键观察
- 同一模型在不同任务中能耗差异显著
- llama2:7b-q8在测试生成中能耗是代码生成的7.2倍
- 任务特性是影响能耗的主要因素

** RQ2: 能耗与准确性的权衡
*** Pareto前沿分析
- 在某些任务中，小模型+量化可实现高精度+低能耗
- 量化模型精度接近甚至优于全精度版本
- 存在"收益递减"现象：大模型能耗增加但精度提升有限

*** 典型案例
- gemma:2b-fp16在文档生成中精度70.12%，能耗仅为大模型的1/4
- phi3:3.8b在代码生成中表现优异，能耗仅为大模型的一半

** RQ3: 架构特征与能耗关联
*** 强相关因素
- 参数数量与能耗强正相关（但与准确性无显著相关）
- 效率指标（tokens/J）与模型大小强负相关
- 量化位数与内存使用、能耗强负相关

*** 架构特征影响
- 注意力头数、前馈网络维度、Transformer块数均与能耗正相关
- 4位量化模型在212/216个案例中能耗最低

** RQ4: 专用vs通用模型表现
*** 代码生成任务
- 代码专用模型明显优于通用模型
- 通用模型中只有llama3:8b进入Pareto前沿

*** 其他任务
- 文档生成和测试生成：通用模型表现竞争力强
- 缺陷修复：所有模型表现均不理想

* 关键洞见与实践建议
** 模型选择策略
- 大模型 ≠ 高精度：参数规模不是准确性的可靠指标
- 任务导向选择：根据具体任务类型选择专用或通用模型
- 量化优先：4位或8位量化版本在保持精度的同时显著降低能耗

** 能耗优化建议
- 关注CPU能耗：即使在GPU推理中，CPU能耗占比可达16%
- 使用效率指标：tokens/J是跨任务比较的有效指标
- 考虑架构特征：参数数量和量化级别是能耗的关键预测因子

** 部署建议
- 资源受限环境：优先选择小模型+量化版本
- 高精度需求：中等规模代码专用模型通常是最优选择
- 多任务场景：需要根据任务特性动态选择模型

* 研究局限性
** 内部有效性威胁
- 提示模板一致性可能影响模型准确性
- 部分模型输出解析困难，可能影响评估
- Ollama API在某些情况下出现不完整响应

** 外部有效性威胁
- 数据集单一：仅使用HumanEvalPack（Python）
- 缺乏真实世界复杂任务的代表性
- 编程语言单一，可能影响结论泛化性

** 比较局限性
- 缺乏严格的通用模型与代码模型配对比较
- 模型选择受硬件和格式兼容性限制

* 结论与未来方向
** 主要结论
- 模型能耗在不同软件开发任务中差异显著
- 能耗与准确性并非总是需要权衡，小模型+量化可达到良好平衡
- 模型架构特征（特别是参数数量和量化级别）是能耗的强预测因子
- 代码专用模型仅在代码生成任务中全面占优

** 未来研究方向
- 扩展多语言、复杂任务评估
- 开发针对非代码生成任务（如缺陷修复）的专用模型
- 探索更多硬件平台与推理框架的能效表现
- 研究动态模型选择策略以适应不同任务需求

* 实践意义
** 对开发者的意义
- 提供基于证据的模型选择指南
- 揭示量化技术在实践部署中的价值
- 强调任务特性在模型选择中的重要性

** 对研究者的意义
- 建立软件开发任务中LLM能耗评估的基准方法
- 为绿色AI和可持续软件开发提供新视角
- 开辟模型架构优化与任务特定调优的研究方向

** 对工具开发者的意义
- 需要开发支持动态模型选择的智能编码助手
- 应提供能耗监控和优化建议功能
- 考虑在不同硬件平台上的部署优化

* 可以提出的问题
"Popularity is measured by the number of downloads... Reputability of creator... While this criterion can be arguably subjective..."
1. 下载量的具体阈值：未说明达到多少下载量才算"流行"
Regarding model selection, you used download count as a measure of "popularity". 
Could you clarify what the actual quantitative threshold was for this?

2.Did the reported energy consumption include the model loading phase, or was it measured strictly during inference?
能耗数据是否包含模型加载阶段，还是仅测量推理阶段？