#+TITLE: The Rise of AI Teammates in Software Engineering (SE) 3.0: How Autonomous Coding Agents Are Reshaping Software Engineering

* 研究背景与动机

** SE 3.0时代的到来
- SE 1.0：传统手工编码时代，无AI辅助
- SE 1.5：预测性编码（代码补全、智能提示）
- SE 2.0：AI辅助软件工程（LLM生成代码）
- SE 3.0：自主AI队友时代，能独立完成端到端开发任务

** 研究动机
- 现有研究多为理论推测，缺乏真实世界中AI队友行为的实证数据
- 需要回答关键问题：
  - 自主编码代理如何影响开发者生产力？
  - AI生成的代码是否可被合并？
  - 人类团队如何适应自主编码代理的存在？
  - 当前基准测试是否能反映AI在真实环境中的表现？

* 主要贡献

** AIDev数据集
- 规模：456,535个由自主编码代理创建的PR
- 覆盖范围：61,453个GitHub仓库，47,303名开发者
- 包含的AI代理：OpenAI Codex、Devin、GitHub Copilot、Cursor、Claude Code
- 数据结构：丰富的元数据，包括PR时间线、代码审查、提交详情、关联问题等

** 实证研究发现
- 系统分析了AI代理在真实项目中的表现、审查动态和代码质量
- 揭示了基准测试表现与现实效果之间的显著差距

** 未来研究方向
- 提出了9个具体的研究方向，指导SE与AI交叉领域的未来探索

* 关键研究发现

** 生产力与接受率
- AI代理已成为不可或缺的协作者，55%+的PR涉及功能开发或修复
- AI-PR接受率显著低于人类（如Codex 64% vs 人类76.8%）
- 文档任务是AI的强项，接受率超过人类基准

** 效率与审查动态
- GitHub Copilot完成速度极快（50%的PR在13分钟内完成）
- OpenAI Codex PR审查速度快10倍（0.3小时 vs 人类3.9小时）
- 人类仍是主要审查者，但机器人审查比例在AI-PR中显著上升

** 代码质量与归属
- AI更倾向于简单、模板化代码，较少引入复杂度变化
- 作者归属不明确，部分AI代理不标注贡献者，影响可追溯性
- AI代理表现出语言偏好（如Codex偏向Python，Copilot偏向C#）

* 未来研究方向

** 基准测试与评估
- 开发基于真实工作流的集成导向基准测试
- 分析被拒PR以识别AI失败模式

** 系统优化
- 延迟感知的AI代理编排
- 设计智能PR分流系统管理审查工作量

** 质量与协作
- 评估AI代码的长期质量
- 研究人-AI协作中的任务规划阶段
- 研究编程语言特性对AI效果的影响

** 审查流程
- 理解并降低审查AI代码的人力成本
- 改进AI代码审查的质量与流程

* 未来展望：SE 3.0方法论

** 软件仓库作为AI训练环境
- 将GitHub仓库视为强化学习环境
- 使用真实信号（PR合并、测试通过）作为奖励

** 动态基准测试
- 取代静态基准测试（如SWE-bench）
- 建立实时排行榜，反映真实项目集成效果

** 新工程方法论
- 需要新的协作框架、审查机制和治理模型
- 重新定义敏捷、DevOps等实践以适应人-AI混合团队

* 结论

- AIDev提供了首个大规模实证基础，证明自主编码代理时代已经到来
- AI代理能显著加速代码贡献，但在接受率和代码复杂性方面与人类存在差距
- 数据集将支持下一代软件工程研究，推动SE 3.0从理论走向实践

* 可以提出的问题：
1. 混淆变量： 研究发现AI-PR的接受率更低。我们如何确定这是因为代码质量差，而不是因为人类审查员对AI生成的代码抱有固有的不信任或更高的审查标准？这一点可能会影响因果推断
Regarding potential confounding variables: The empirical findings indicate a systematically lower acceptance rate for Agentic-PRs. 
How can we ascertain that this is primarily due to inferior code quality, as opposed to inherent distrust or a heightened scrutiny standard applied by human reviewers to AI-generated code? 
This ambiguity presents a challenge to establishing a clear causal interpretation of the results.

#+TITLE: Is Refactoring Always a Good Egg? Exploring the Interconnection Between Bugs and Refactorings
本文通过实证研究深入探讨了代码重构与软件缺陷之间的复杂关系。

* 研究背景与动机

** 传统认知
- Bug修复：纠正性修改，旨在消除程序缺陷
- 代码重构：行为保持的代码改进，旨在提升内部质量而不改变功能
- 传统观点认为这两种活动应该独立进行，且重构不应引入缺陷

** 研究动机
- 实际开发中，开发者对重构存在顾虑，担心引入缺陷
- 现有研究对重构与缺陷关系的结论不一致
- 需要基于大规模实证数据探究重构与缺陷的真实关系

* 研究问题

** RQ1：Bug修复提交中是否常见混杂重构更改？
- 探究开发者在修复缺陷时是否同时进行重构

** RQ2：重构操作是否出现在引入缺陷的代码修改中？
- 分析重构是否真的可能引入新的缺陷

** RQ3：在引入缺陷的提交中，哪些重构类型最为常见？
- 识别高风险的重构类型，为开发者提供预警

* 研究方法

** 数据来源
- SmartSHARK 2.2 数据集
- 涵盖96个Java项目
- 包含提交标签、重构操作、缺陷引入信息

** 分析工具
- 重构检测：RMiner工具（精度98%，召回率87%）
- 缺陷引入识别：基于SZZ算法
- 提交级别关联分析

** 分析方法
- 通过提交ID关联重构与缺陷记录
- 统计共现频率
- 识别高风险重构类型

* 主要研究发现

** RQ1：Bug修复提交中的重构混杂情况
- 41/96个项目存在重构与bug修复混杂的提交
- 平均21%的bug修复提交包含重构操作
- 最高比例：Calcite项目（41%）
- 但仅有10%的重构操作与bug修复混杂

** RQ2：重构在缺陷引入提交中的出现情况
- 平均54%的缺陷引入提交包含重构操作
- 比例范围：20%（commons-validator）到71%（Calcite）
- 重构与缺陷引入存在显著共现关系

** RQ3：高风险重构类型识别

*** 高频出现类型（按次数）
- Change Variable Type（652次）
- Extract Method（454次）
- Change Return Type（338次）

*** 高风险类型（按比例R1%）
- Extract Subclass（33%出现在缺陷引入提交中）
- Replace Attribute（29%）
- Move and Rename Attribute（28%）

* 结论与启示

** 主要结论
- 重构与缺陷活动在实践中并非独立
- 重构并非总是"安全"的行为保持操作
- 特定重构类型具有较高风险

** 实践启示
- 重构时应加强验证与测试，特别是高风险类型
- 避免在修复缺陷时混杂不相关的重构
- 提高对重构潜在风险的认识

** 研究局限性
- 基于提交级别的共现分析，未建立因果关系
- 依赖检测工具的准确性（RMiner、SZZ）
- 数据集局限于Java项目

* 未来工作

** 定性研究
- 深入分析重构与缺陷之间的因果关系
- 探究重构引入缺陷的根本机制

** 扩展研究
- 扩展到更多编程语言和项目类型
- 开发重构风险评估工具
- 建立重构最佳实践指南

* 总结

本研究通过实证分析挑战了"重构总是安全"的传统观念，揭示了重构与缺陷之间的复杂关联，为开发者理解和管理重构风险提供了重要依据。

* 可以提出的问题：
“最安全”的重构是什么？ 论文重点指出了高风险重构，但从表3看，像“移动类”这样的重构在缺陷引入提交中出现的比例较低（5%）。这是否意味着某些重构实际上是相对安全的？
The paper prominently highlights high-risk refactoring types. 
However, as seen in Table 3, refactorings like Move Class have a relatively low presence in bug-inducing commits (R1% of only 5%). 
Does this imply that certain refactoring operations are, in fact, relatively safe? 
What patterns or characteristics might these lower-risk refactorings share, and could this inform the development of safer refactoring practices?

#+TITLE: 论文笔记：An Empirical Study of End-user Programmers in the Computer Music Community
#+AUTHOR: Gregory Burlet, Abram Hindle
#+DATE: 2015

* 摘要
计算机音乐家是使用可视化编程语言（如 Max/MSP, Pure Data）的**终端用户程序员**。
想象一下，传统的音乐家弹的是钢琴、吉他，而计算机音乐家“弹”的是电脑。他们不只是用现成的音乐软件（比如GarageBand）来拖拽音效，而是像一个程序员一样，通过“连接积木”的方式来从零开始创造声音和音乐。
-“积木”：就是软件里的各种小模块，有的负责发出声音（比如一个模拟正弦波的振荡器），有的负责控制音量，有的负责接收键盘或MIDI控制器的信号。
-“连接”：就是用虚拟的线把这些模块连起来，让数据（比如音频信号、控制信号）在它们之间流动。
他们使用的“画布”就是像 Max/MSP 或 Pure Data 这样的可视化编程语言。所以，他们既是音乐家，也是程序员，我们称他们为 “用电脑作曲的程序员”。


本研究通过多层面分析揭示了他们的开发实践：
1.  与普通开发者相比，他们的代码库有：**更少提交**、**提交频率更低**、**更多周末提交**，但**问题报告数和贡献者数相似**。（“他们更随性，更像是周末艺术家”）
2.  源代码分析发现，绝大部分代码可由重复的代码片段重建。（“他们的代码‘复制粘贴’率极高”）
3.  调查和访谈结果佐证了上述发现。（“他们不太爱用‘高级工具’，但协作精神不差”）
结论：软件工程有许多途径可以帮助这个终端用户程序员社区。

这篇论文就像一个“针对‘用电脑作曲的程序员’这个特殊群体的社会调查报告”。它想搞清楚一个问题：这群既是艺术家又是程序员的人，他们的工作方式和普通的软件工程师有什么不同？

* 1. 引言
** 背景
- 计算机音乐家是**终端用户程序员**，使用可视化编程语言（如 Max/MSP, Pure Data）创作音乐。
- 他们面临与专业程序员相似的软件工程挑战（如数据流、调试、测试、API调用），但技术能力参差不齐。
- 可视化音乐编程语言通过排列**对象**（Objects）并用**连线**（Patchcords）连接它们来工作。一个文件称为一个**程序**（Patch）。

** 研究问题
- 我们不清楚计算机音乐家如何编程、分享和协作。
- 他们是否使用软件工程工具（如版本控制、问题追踪器）？
- 他们在开发过程中遇到什么问题？现有的软件工程实践和工具能否帮助他们？

* 2. 相关工作
** 2.1 终端用户可视化编程
- 终端用户程序员数量庞大，其目标是实现个人领域内的目标，而非受雇开发软件。
- 计算机音乐家属于终端用户，他们使用专门的、音乐导向的编程语言进行个人创作。
- 可视化编程语言通过图形化隐喻降低学习曲线，并提供实时反馈。
- 已有对其他终端用户社区（如Yahoo! Pipes, CoScripter, 电子表格用户）的研究，但对计算机音乐社区尚无实证研究。

** 2.2 软件仓库挖掘
- GitHub 是流行的公开软件项目集合。
- GHTorrent 项目提供了从 GitHub 提取的数据集，用于研究。

** 2.3 克隆检测
- 克隆检测可用于促进代码重用、为新手提供参考、定位需要重构的代码。
- 已有研究对类似的可视化编程语言（如Matlab Simulink）进行克隆检测。
- Gold et al. (2011) 对 Max/MSP 教程程序进行了克隆检测，发现86%的连接对象是最低粒度的克隆。但未研究社区开发的程序。

* 3. 挖掘 Git 仓库
** 目标：分析计算机音乐家与普通开发者在开发实践上的差异。

** 3.1 数据集
- **计算机音乐数据集**：从GHTorrent中查询得到819个主要包含Max/MSP或Pure Data文件的仓库。
  | 语言    | 仓库数 | 程序数  | 对象数   |
  |---------|--------|---------|----------|
  | Max/MSP | 168    | 15,016  | 565,705  |
  | Pure Data | 651  | 103,465 | 2,521,573 |
  | 总计    | 819    | 118,481 | 3,087,278 |
- **随机样本数据集**：从GHTorrent中随机抽取819个仓库作为对照。

** 3.2 假设与访谈反馈
针对以下5个假设，访谈了15位计算机音乐家：
1.  H1: 提交次数更少 :: 7同意，1反对，7不确定
2.  H2: 周末提交更多 :: 4同意，2反对，9不确定
3.  H3: 提交频率更低 :: 5同意，1反对，9不确定
4.  H4: 问题报告更少 :: 7同意，3反对，5不确定
5.  H5: 贡献者更少   :: 8同意，1反对，6不确定

** 3.4 结果
使用Wilcoxon秩和检验与Cliff's Delta效应量进行验证 (α=0.01)：
| 假设 | 结果 | p值 | 效应量 | 结论 |
|------|------|-----|--------|------|
| H1: 提交次数更少 | ✅ 支持 | 1.133e-13 | 小 | 计算机音乐家提交更少 |
| H2: 周末提交更多 | ✅ 支持 | 7.091e-5 | 小 | 计算机音乐家周末提交更多 |
| H3: 提交频率更低 | ✅ 支持 | ≈0 | 中等 | 计算机音乐家提交频率更低 |
| H4: 问题报告更少 | ❌ 拒绝 | 0.214 | 可忽略 | 两者问题报告数无差异 |
| H5: 贡献者更少 | ❌ 拒绝 | 0.4673 | 可忽略 | 两者贡献者数无差异 |

** 总结：计算机音乐家提交更少、更不频繁、更多在周末，但问题报告和协作程度与普通开发者无显著差异。

* 4. 克隆检测
** 目标：分析计算机音乐家代码中的重复结构。

** 4.1 克隆检测算法
- 定义了两种克隆粒度：
  - **DF1克隆**：子图对象类型、字面值参数、连接方式完全相同。
  - **DF2克隆**：子图对象类型、连接方式相同，参数可不同。
- 算法：将程序解析为图，深度优先遍历（深度≤8），提取路径属性，生成JSON文本并哈希，通过哈希比较检测克隆。
- 验证：在Gold et al.使用的教程程序数据集上验证，结果相似。

** 4.3 结果
在118,481个程序中发现：
| 克隆类型 | 克隆数量 | 路径总数 | 克隆比例 |
|----------|----------|----------|----------|
| DF1      | 9,798,031 | 10,985,064 | 89.2% |
| DF2      | 10,462,725 | 10,985,064 | 95.2% |
- 结论：计算机音乐家的代码重复率极高。

** 4.4 值得注意的克隆示例
- (a) 包络跟随器去归一化
- (b) 触发多个bang的loadbang
- (c) Pure Data中模拟loadmess
- (d) 恒等函数（可能为预留功能）
- (e) 过于简单的函数（如衰减）
- (f) 无操作参数（如乘1）或静音参数
- (g) 单行表达式 vs 链式数学运算
- (h) 高通+低通滤波器（可用现成的带通滤波器替代）
- (i) 使用外部对象库简化常见功能
- (j) 使用魔数（如MIDI最大值127）和数学常数（如π）

** 工具启示：可开发代码补全、代码审查、代码搜索等工具来帮助计算机音乐家。

* 5. 计算机音乐家调查与访谈
调查了175位计算机音乐家，访谈了15位。

** 5.1 经验
- 经验年限分布广泛。
- 自评水平：15初学者，84中级，74高级。

** 5.2 动机
- 65% 作为业余爱好。
- 40% 为他人或公司开发。
- 访谈：音乐项目通常高度个人化，遵循单一音乐家的创作愿景。

** 5.3 编程方法与版本控制
- **常用语言**：Max/MSP 和 Pure Data 是最常用的前两名。
- **编写测试**：仅30%编写测试。可能因声音属性难以量化断言。
- **代码注释**：大多数会注释关键部分。数据集中有16.4%的对象是注释对象。
- **使用版本控制**：54%使用。前五名版本控制系统中有四个是Git。
- **不使用版本控制的原因**：
  - 还在学习如何使用
  - 觉得没必要（版本备份未造成大问题）
  - 代码不与他人协作，版本管理非优先事项

** 5.4 开发支持
- 仅26%使用Stack Overflow。
- 54%订阅邮件列表。
- 求助路径：先使用搜索引擎，再通过邮件列表求助社区。
- 启示：社区可能受益于一个专门的问答网站。

* 6. 有效性威胁
- **抽样偏差**：GitHub上的公共仓库可能不能代表所有计算机音乐家；随机样本也未过滤非软件项目。
- **克隆检测**：混合分析Max/MSP和Pure Data可能因同名对象功能不同导致克隆数被低估。
- **调查与访谈**：招募渠道可能偏向特定子社区。

* 7. 未来方向
- **开发专用工具**：
  - 代码补全
  - 代码审查
  - 代码搜索
  - 代码高亮与导航
  - 可视化代码聚类
  - 音乐软件测试框架
- **改进版本控制**：适配可视化、基于程序的源代码的差异比较与合并。
- **教育**：向计算机音乐家普及软件工程工具和方法论的价值与使用。

* 8. 结论
- **开发实践**：计算机音乐家提交更少、更不频繁、更多在周末，但问题报告和协作程度与普通开发者无差异。
- **代码重复**：代码重复率极高（DF1: 89.2%, DF2: 95.2%），许多克隆是现有功能的重复实现。
- **工具使用**：许多人不用版本控制，缺乏专用的支持网站，依赖邮件列表。
- **未来工作**：需要教育和开发工具来帮助这个终端用户社区及其他使用可视化编程语言的社区。

可以提出的问题：
1.What were the criteria for selecting the analyzed music software projects? 
Were they chosen based on popularity, functionality, or language similarity?

2. How was the “computer music repository” dataset specifically identified and filtered from GHTorrent? 
Were any manual validation steps taken to ensure repositories were indeed music-related and actively developed?
计算机音乐仓库的数据集是如何从 GHTorrent 中具体识别和筛选的？是否有人工验证步骤来确保这些仓库确实与音乐相关且处于活跃开发状态？

3. Why was the path depth limited to 8 in the clone detection algorithm?
Was this value empirically determined, and how might it affect the detection of larger clone structures?
为什么在克隆检测算法中将路径深度限制为 8？这个值是经验确定的吗？它如何影响对更大克隆结构的检测？

#+TITLE: Can LLMs Replace Manual Annotation of Software Engineering Artifacts?
#+AUTHOR: Toufique Ahmed, Premkumar Devanbu, Christoph Treude, Michael Prade
#+DATE: 2024

* 核心问题
** 能否用大语言模型来替代昂贵的人工标注，用于软件工程产物的评估？

* 摘要
- **背景**：软件工程研究中的人工实验成本高昂（例如，聘请专业开发者每小时60美元），且难以执行。
- **机遇**：大语言模型在多项任务上展现出接近人类的能力，且调用成本极低。
- **研究**：将6个先进的LLM应用于10个来自5个数据集的标注任务，比较LLM与人类标注员之间的一致性。
- **发现**：
  1. 在一些任务上，LLM与人类的一致性接近人类之间的一致性。
  2. **模型间一致性** 可以用来预测一个任务是否适合使用LLM。
  3. **模型置信度** 可以用来选择LLM能安全替代人类的特定样本。
- **结论**：LLM可以部分替代人工标注，形成一个**混合的人机评估**模式，显著降低成本。

* 1. 引言
** 研究动机
- 软件工程工具（如代码摘要、静态分析）的最终价值取决于人类的判断。
- 人工评估成本高、耗时长，且难以找到代表性样本（如专业开发者）。
- LLMs的出现提供了一个潜在的低成本替代方案。

** 核心研究问题
- When, and how, can human subject responses be safely replaced by LLMs, in a mixed human-LLM evaluation scenario?

* 2. 方法论
** 2.1 任务与数据集
选择了5个数据集，涵盖10个标注任务：
1. 自动代码摘要 :: 评估生成摘要的准确性、充分性、简洁性、相似性。
2. 名称-值一致性 :: 判断变量名与其值是否匹配。
3. 因果关系提取 :: 从需求文档中判断句子是否包含因果关系。
4. 语义相似性 :: 判断两个函数在目标、操作、效果上是否相似。
5. 静态分析警告 :: 判断代码变更是否真正修复了静态分析警告。

** 2.2 使用的模型
- **闭源模型**：GPT-4, Claude-3.5-Sonnet, Gemini-1.5-Pro, GPT-3.5
- **开源模型**：Llama3 (70B), Mixtral (8x22B)

** 2.3 研究问题
- RQ1 :: LLM与人类标注员的一致性水平如何？
- RQ2 :: 如何判断一个任务是否不适合使用LLM？
- RQ3 :: 如何判断LLM对单个样本的回答是否可靠？
- RQ4 :: 能节省多少人工标注成本？

** 2.4 评估方法
- 使用 **Krippendorff‘s α** 作为一致性度量指标。
- 比较三类一致性：
  - Human-Human (H-H)
  - Human-Model (H-M)
  - Model-Model (M-M)

* 3. 结果与发现
** RQ1: LLM与人类的一致性水平
| 任务类别             | H-H一致性 | H-M一致性 | 结论                                     |
|----------------------|-----------|-----------|------------------------------------------|
| 代码摘要（多数任务） | 中-高     | 与H-H相当 | LLM可作为有效替代                        |
| 名称-值一致性        | 高        | 与H-H相当 | LLM可作为有效替代                        |
| 语义相似性           | 高        | 高        | LLM可作为有效替代                        |
| 因果关系提取         | 中        | 低        | LLM表现不佳                             |
| 静态分析警告         | 高        | 低        | LLM表现不佳                             |

** RQ2: 如何判断任务是否适合LLM？
- **关键发现**：Model-Model一致性与Human-Model一致性**强正相关**。
- **实践指南**：可以先计算多个LLM在目标任务上的M-M一致性。
  - 如果 > 0.5 → 任务适合使用LLM。
  - 如果 ≤ 0.5 → 任务不适合或需谨慎使用。

** RQ3: 如何判断单个样本的LLM回答是否可靠？
- **关键发现**：LLM的**输出概率（置信度）** 是一个有效的指示器。
- **实践指南**：优先选择LLM输出置信度高的样本来替代人工标注，这比随机选择能更好地维持整体一致性。

** RQ4: 能节省多少人力？
- 通过有选择地用LLM替代**每个样本的一个**人类标注，可以在**7/10个任务**中保持原有的一致性水平。
- 总体标注工作量**最高可节省约33%**。
| 数据集/任务          | 可节省的总体工作量 |
|----------------------|--------------------|
| 代码摘要（多数任务） | 33%                |
| 代码摘要（相似性）   | 16.5%              |
| 名称-值一致性        | 9%                 |
| 因果关系             | 30%                |
| 语义相似性           | 33%                |
| 静态分析警告         | 25%                |

* 4. 讨论与建议
** 提出的决策流程
1. 用少量示例（3-4个）测试多个LLM，计算**模型间一致性**。
   - 若 > 0.5 → 可安全地用LLM替代**每个样本的一个**人类标注。
   - 若 ≤ 0.5 → 进入下一步。
2. 对于一致性低的任务，仅对LLM**输出置信度高**的样本使用其标注结果。

** 重要警示
- **不应完全替代所有人类**：用LLM完全取代所有人类标注员可能会人为地提高一致性，但无法反映真实的人类认知差异。
- **少量示例学习至关重要**：零样本学习效果不佳。

* 5. 局限性
- 只研究了10个任务和6个模型。
- 某些数据集的标注员数量少或数据分布不均。
- 存在LLM在训练中已见过部分测试数据的风险。
- 未研究自由形式的标注任务，也未探讨模型输出偏差或人口统计学问题。

* 6. 结论
- LLM可以部分替代人工标注，形成一个**混合的人机评估**模式。
- **模型间一致性**是判断任务适用性的有效预测指标。
- **模型置信度**是选择可靠样本的有效方法。
- 这项工作是迈向软件工程中混合人-LLM评估的第一步。

* 可以提出的问题：
1.In RQ3/RQ4, the conclusion that inter-rater agreement does not change "significantly" is crucial. 
What statistical test was used to determine significance, and what was the p-value threshold? This is not explicitly stated.
在RQ3/RQ4中，“评估者间一致性没有发生‘显著’变化”这一结论至关重要。是使用了哪种统计检验来确定显著性的，p值的阈值是多少？这一点没有明确说明。

2.The prompts for different tasks (especially complex ones like static analysis warnings) likely varied in structure and detail. 
How was the prompt engineering process conducted to ensure fairness and optimal performance across all tasks and models? 
Were prompts optimized for each model?
针对不同任务（特别是像静态分析警告这样的复杂任务）的提示词，其结构和细节很可能不同。
提示词工程的过程是如何进行的，以确保在所有任务和模型间的公平性和最佳性能？提示词是否为每个模型单独优化过？